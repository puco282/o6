import streamlit as st
from openai import OpenAI # Updated import
from PIL import Image
import base64
import io

# OpenAI API í‚¤ ì„¤ì • (Streamlit Secretsì—ì„œ ê°€ì ¸ì˜´)
# Set OpenAI API key (retrieved from Streamlit Secrets)
# Initialize the OpenAI client
client = OpenAI(api_key=st.secrets["openai"]["api_key"])


st.set_page_config(page_title="Pika ì˜ìƒ ì œì‘ GPT ë„ìš°ë¯¸")
st.title("ğŸ¬ Pika ì˜ìƒ ì œì‘ GPT ë„ìš°ë¯¸")

# ì‚¬ì´ë“œë°”ì—ì„œ ì‘ì—… ì„ íƒ
# Select task from sidebar
chat_option = st.sidebar.radio("ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”:", [
    "1. ì´ì•¼ê¸° ì ê²€í•˜ê¸°",
    "2. ì´ì•¼ê¸° ë‚˜ëˆ„ê¸°",
    "3. ìºë¦­í„°/ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±",
    "4. ì¥ë©´ë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ì ê²€"
])

# ê³µí†µ GPT í˜¸ì¶œ í•¨ìˆ˜
# Common GPT call function
def ask_gpt(messages, model="gpt-4o"):
    # Updated API call using the new client syntax
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    return response.choices[0].message.content

# ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ (DALLÂ·E ì‚¬ìš©)
# Image generation function (using DALLÂ·E)
def generate_image(prompt):
    # Updated API call using the new client syntax
    response = client.images.generate(
        model="dall-e-3",
        prompt=prompt,
        size="1024x1024",
        response_format="b64_json"
    )
    # The response structure changed in v1.0.0
    image_data = base64.b64decode(response.data[0].b64_json)
    return Image.open(io.BytesIO(image_data))

# 1. ì´ì•¼ê¸° ì ê²€í•˜ê¸°
# 1. Story Review
if chat_option.startswith("1"):
    st.header("1. ì´ì•¼ê¸° ì ê²€í•˜ê¸°")
    st.markdown("ğŸ’¬ **ëª©í‘œ:** ì—¬ëŸ¬ë¶„ì˜ ì´ì•¼ê¸°ê°€ ì˜ìƒìœ¼ë¡œ ë§Œë“¤ê¸°ì— ì ì ˆí•œì§€ GPTì™€ í•¨ê»˜ ì ê²€í•˜ê³  ë‹¤ë“¬ì–´ ë³´ì„¸ìš”.")
    story = st.text_area("ì—¬ëŸ¬ë¶„ì´ ì°½ì‘í•œ ì´ì•¼ê¸°ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    if st.button("GPTì—ê²Œ ì ê²€ë°›ê¸°") and story:
        messages = [
            {"role": "system", "content": (
                "ë„ˆëŠ” ì´ˆë“±í•™ìƒì´ ì°½ì‘í•œ ì´ì•¼ê¸°ë¥¼ ë³´ê³ , Pika ì˜ìƒìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ì ê²€í•´ì£¼ëŠ” GPT ë„ìš°ë¯¸ì•¼.\n"
                "ë¨¼ì € í•™ìƒì˜ ì´ì•¼ê¸°ë¥¼ ì½ê³ , í‘œí˜„í•˜ê³ ì í•˜ëŠ” ì˜ë„ê°€ ëª…í™•í•œì§€ íŒŒì•…í•´.\n"
                "ì´í•´ë˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì§ˆë¬¸ì„ í†µí•´ ëª…í™•í™”í•˜ë„ë¡ ìœ ë„í•´.\n"
                "ì†Œí¬ë¼í…ŒìŠ¤ì‹ ì§ˆë¬¸ìœ¼ë¡œ í•™ìƒ ìŠ¤ìŠ¤ë¡œ í‘œí˜„ì„ ë‹¤ë“¬ë„ë¡ ë„ì™€ì¤˜.\n"
                "ì´ì•¼ê¸°ì˜ ê¸¸ì´(1~1ë¶„ 30ì´ˆ), êµ¬ì¡°(ë°œë‹¨-ì „ê°œ-ì ˆì •-ê²°ë§), ì–´ìƒ‰í•œ ë¬¸ì¥ì€ ë¶€ë¶„ ì˜ˆì‹œ ì¤‘ì‹¬ìœ¼ë¡œ í”¼ë“œë°±í•˜ê³ ,\n"
                "ë§ˆì§€ë§‰ì—ëŠ” ì¢…í•©ì ì¸ ë³´ì™„ ë°©í–¥ì„ ê²©ë ¤í•˜ëŠ” ë§íˆ¬ë¡œ ì•ˆë‚´í•´ì¤˜."
            )},
            {"role": "user", "content": story}
        ]
        with st.spinner("GPTê°€ ì´ì•¼ê¸°ë¥¼ ì ê²€ ì¤‘ì…ë‹ˆë‹¤..."):
            st.write(ask_gpt(messages))

# 2. ì´ì•¼ê¸° ë‚˜ëˆ„ê¸°
# 2. Story Segmentation
elif chat_option.startswith("2"):
    st.header("2. ì´ì•¼ê¸° ë‚˜ëˆ„ê¸° (ì¥ë©´ ë¶„í• )")
    st.markdown("âœ‚ï¸ **ëª©í‘œ:** ê¸´ ì´ì•¼ê¸°ë¥¼ 10ì´ˆ ë‚´ì™¸ì˜ ì§§ì€ ì˜ìƒ ì¥ë©´ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ì¥ë©´ì˜ í•µì‹¬ ìš”ì†Œë¥¼ ëª…í™•íˆ í•´ë³´ì„¸ìš”.")

    if "scenes" not in st.session_state:
        st.session_state.scenes = {}
    if "scene_count" not in st.session_state:
        st.session_state.scene_count = 1

    # ì¥ë©´ ì…ë ¥ í•„ë“œ
    # Scene input fields
    for i in range(1, st.session_state.scene_count + 1):
        st.session_state.scenes[f"part_{i}"] = st.text_area(
            f"ì¥ë©´ {i} ì…ë ¥",
            value=st.session_state.scenes.get(f"part_{i}", ""),
            key=f"part_{i}"
        )
        if st.session_state.scenes[f"part_{i}"]:
            messages = [
                {"role": "system", "content": (
                    "ë„ˆëŠ” í•™ìƒì˜ ì´ì•¼ê¸°ë¥¼ 10ì´ˆ ë‚´ì™¸ë¡œ ë‚˜ëˆ ì„œ Pika ì˜ìƒ ì¥ë©´ìœ¼ë¡œ êµ¬ì„±í•˜ë„ë¡ ë•ëŠ” ì¡°ë ¥ìì•¼.\n"
                    "ê° ì¥ë©´ì—ì„œ ì‹œê°„, ì¥ì†Œ, ë“±ì¥ì¸ë¬¼, ì‚¬ê±´ì„ íŒŒì•…í•˜ê³ , ì´ì „ ì¥ë©´ê³¼ êµ¬ë¶„ë˜ëŠ” ìš”ì†Œì¸ì§€ ì§ˆë¬¸ì„ í†µí•´ í™•ì¸í•´.\n"
                    "ì´ë¯¸ ì…ë ¥ëœ ì •ë³´ëŠ” ë¬»ì§€ ì•Šê³ , ëˆ„ë½ëœ ì •ë³´ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ì§ˆë¬¸í•´ì¤˜.\n"
                    "í•„ìš”í•˜ë‹¤ë©´ ì´ ì¥ë©´ì„ ë‚˜ëˆ„ê±°ë‚˜ ì• ì¥ë©´ê³¼ í•©ì¹˜ëŠ” ê²ƒì´ ë” ìì—°ìŠ¤ëŸ¬ìš´ì§€ë„ ì•Œë ¤ì¤˜."
                )},
                {"role": "user", "content": st.session_state.scenes[f"part_{i}"]}
            ]
            st.markdown(f"**GPT í”¼ë“œë°± (ì¥ë©´ {i})**")
            with st.spinner(f"GPTê°€ ì¥ë©´ {i}ë¥¼ ì ê²€ ì¤‘ì…ë‹ˆë‹¤..."):
                st.write(ask_gpt(messages))
        st.markdown("---") # ê° ì¥ë©´ êµ¬ë¶„ì„ ìœ„í•œ ì‹œê°ì  êµ¬ë¶„ì
                             # Visual separator for each scene

    col1, col2 = st.columns(2)
    with col1:
        if st.session_state.scene_count < 9: # ìµœëŒ€ 9ê°œ ì¥ë©´ ì œí•œ
                                             # Maximum 9 scenes limit
            if st.button("ìƒˆ ì¥ë©´ ì¶”ê°€"):
                st.session_state.scene_count += 1
                st.experimental_rerun()
    with col2:
        if st.button("ëª¨ë“  ì¥ë©´ ì…ë ¥ ì™„ë£Œ ë° ìµœì¢… í”¼ë“œë°± ë°›ê¸°"):
            all_scenes_content = "\n".join([
                f"ì¥ë©´ {i}: {st.session_state.scenes[f'part_{i}']}"
                for i in range(1, st.session_state.scene_count + 1)
                if st.session_state.scenes.get(f'part_{i}')
            ])

            if all_scenes_content.strip(): # ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ë°©ì§€
                                          # Prevent case where only whitespace exists
                final_feedback_messages = [
                    {"role": "system", "content": (
                        "ë„ˆëŠ” ì´ˆë“±í•™ìƒì´ ë‚˜ëˆˆ ì´ì•¼ê¸° ì¥ë©´ë“¤ì˜ ì „ì²´ì ì¸ íë¦„ì„ ê²€í† í•˜ê³  ìµœì¢…ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” GPT ë„ìš°ë¯¸ì•¼.\n"
                        "í•™ìƒì´ ì…ë ¥í•œ ì „ì²´ ì¥ë©´ë“¤ì„ ë³´ê³ , ì´ì•¼ê¸°ì˜ íë¦„ì´ ìì—°ìŠ¤ëŸ¬ìš´ì§€, ë¹ ì§€ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” ë¶€ë¶„ì€ ì—†ëŠ”ì§€ í™•ì¸í•´ì¤˜.\n"
                        "ê° ì¥ë©´ì´ Pikaë¥¼ ì‚¬ìš©í•œ 10ì´ˆ ë‚´ì™¸ì˜ ì˜ìƒìœ¼ë¡œ êµ¬ì„±í•˜ê¸°ì— ì ì ˆí•œì§€, ì „ì²´ì ìœ¼ë¡œ 1ë¶„~1ë¶„ 30ì´ˆ ë¶„ëŸ‰ì˜ ì˜ìƒì´ ë‚˜ì˜¬ ìˆ˜ ìˆì„ì§€ ì¡°ì–¸í•´ì¤˜.\n"
                        "í•„ìš”í•˜ë‹¤ë©´ ì¥ë©´ì˜ ìˆœì„œ ì¡°ì •ì´ë‚˜ í†µí•©, ì¬ë¶„í• ì— ëŒ€í•œ ì¡°ì–¸ì„ ê²©ë ¤í•˜ëŠ” ë§íˆ¬ë¡œ ì œì‹œí•´ì¤˜."
                    )},
                    {"role": "user", "content": f"í•™ìƒì´ ë‚˜ëˆˆ ì „ì²´ ì´ì•¼ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{all_scenes_content}\n\nì´ ì „ì²´ ì´ì•¼ê¸°ì— ëŒ€í•´ ìµœì¢…ì ìœ¼ë¡œ ì ê²€í•˜ê³  í”¼ë“œë°±ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."}
                ]
                st.subheader("ğŸ’¡ ìµœì¢… ì´ì•¼ê¸° íë¦„ ì ê²€ GPT í”¼ë“œë°±")
                with st.spinner("GPTê°€ ì „ì²´ ì´ì•¼ê¸°ë¥¼ ì ê²€ ì¤‘ì…ë‹ˆë‹¤..."):
                    st.write(ask_gpt(final_feedback_messages))
            else:
                st.warning("ë¨¼ì € í•˜ë‚˜ ì´ìƒì˜ ì¥ë©´ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# 3. ì´ë¯¸ì§€ ìƒì„±
# 3. Image Generation
elif chat_option.startswith("3"):
    st.header("3. ìºë¦­í„°/ë°°ê²½ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ìƒì„±")
    st.markdown("ğŸ¨ **ëª©í‘œ:** Pika ì˜ìƒì— ì‚¬ìš©í•  ìºë¦­í„°ë‚˜ ë°°ê²½ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ í”„ë¡¬í”„íŠ¸ë¥¼ GPTì™€ í•¨ê»˜ êµ¬ì²´í™”í•˜ê³  ì§ì ‘ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ ë³´ì„¸ìš”.")
    st.markdown("**ğŸ’¡ íŒ:** Pika ì˜ìƒì— ì“°ì¼ ëŒ€í‘œ ì´ë¯¸ì§€ì´ë¯€ë¡œ, **ìºë¦­í„°ëŠ” ê¼­ ë°°ê²½ ì—†ì´ ì „ì‹ **ìœ¼ë¡œ, **ë‹¨ìˆœí•˜ê³  ëª…í™•í•˜ê²Œ** ë¬˜ì‚¬í•˜ëŠ” ê²ƒì´ ì¢‹ì•„ìš”!")

    if "image_history" not in st.session_state:
        st.session_state.image_history = []
        st.session_state.current_prompt = ""

    concept = st.text_area("ë§Œë“¤ê³  ì‹¶ì€ ìºë¦­í„°ë‚˜ ë°°ê²½ì„ ì„¤ëª…í•´ì£¼ì„¸ìš” (ì˜ˆ: ìˆ² ì†ì„ íƒí—˜í•˜ëŠ” ìš©ê°í•œ ì†Œë…„)")
    if st.button("GPTì—ê²Œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ìš”ì²­") and concept:
        system_prompt = (
            "ë„ˆëŠ” í•™ìƒì˜ ìƒìƒì„ ë°”íƒ•ìœ¼ë¡œ Pika ì˜ìƒì— ì‚¬ìš©í•  ìºë¦­í„°ë‚˜ ë°°ê²½ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•´ì£¼ëŠ” GPTì•¼.\n"
            "ì´ ì´ë¯¸ì§€ëŠ” Pikaì—ì„œ ì˜ìƒì„ ìƒì„±í•  ë•Œ ì‚¬ìš©í•  ëŒ€í‘œ ì´ë¯¸ì§€ì„ì„ ì¸ì§€í•˜ê³ , ê³¼ë„í•œ ë¬˜ì‚¬ ì—†ì´ í•µì‹¬ íŠ¹ì§•ì„ ì˜ ë‚˜íƒ€ë‚´ë„ë¡ ë„ì™€ì¤˜.\n"
            "ì…ë ¥ëœ ì„¤ëª…ì— í¬í•¨ë˜ì§€ ì•Šì€ ì •ë³´(ëŒ€ìƒ êµ¬ë¶„(ìºë¦­í„°/ë°°ê²½), ìŠ¤íƒ€ì¼(ì˜ˆ: ìŠ¤ëˆ„í”¼ íœí™”, ë””ì¦ˆë‹ˆí’), ì™¸í˜•, í‘œì •, ìì„¸ ë“±)ë¥¼ ì§ˆë¬¸í•´ì„œ ë³´ì™„í•´ì¤˜.\n"
            "íŠ¹íˆ, **ìºë¦­í„° ì´ë¯¸ì§€ëŠ” ë°˜ë“œì‹œ ë°°ê²½ ì—†ì´ ì „ì²´ ëª¸ì´ ë³´ì´ë„ë¡** í•´ì•¼ í•˜ê³ , ë‹¤ì–‘í•œ êµ¬ë„ë³´ë‹¤ëŠ” ìºë¦­í„°ì˜ íŠ¹ì§•ì„ ì˜ ë³´ì—¬ì£¼ëŠ” í•˜ë‚˜ì˜ ëª…í™•í•œ ì´ë¯¸ì§€ë¥¼ ëª©í‘œë¡œ í•´.\n"
            "ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ë©´, ë§ˆì§€ë§‰ì— 'ì™„ì„±ëœ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”:' í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ì—¬ì£¼ê³ ,\n"
            "ê·¸ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ì„œ ë³´ì—¬ì¤˜."
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": concept}
        ]
        with st.spinner("GPTê°€ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            gpt_response = ask_gpt(messages)
        st.write(gpt_response)

        if "ì™„ì„±ëœ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”:" in gpt_response:
            prompt_line = gpt_response.split("ì™„ì„±ëœ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”:")[-1].strip()
            # í”„ë¡¬í”„íŠ¸ ì•ë’¤ì˜ ë”°ì˜´í‘œë‚˜ ê³µë°± ì œê±°
            # Remove quotes or spaces from the beginning/end of the prompt
            prompt = prompt_line.strip("'").strip("\"")
            st.session_state.current_prompt = prompt
            with st.spinner("ì´ë¯¸ì§€ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                image = generate_image(prompt)
            st.session_state.image_history = [(prompt, image)]

    # ì´ë¯¸ì§€ í”¼ë“œë°± ë£¨í”„
    # Image feedback loop
    if st.session_state.image_history:
        prompt, image = st.session_state.image_history[-1]
        st.image(image, caption="ìƒì„±ëœ ì´ë¯¸ì§€", use_column_width=True)

        buf = io.BytesIO()
        image.save(buf, format="PNG")
        st.download_button(
            label="ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
            data=buf.getvalue(),
            file_name="generated_image.png",
            mime="image/png"
        )

        st.subheader("ì´ë¯¸ì§€ í™•ì¸ ë° ìˆ˜ì •")
        feedback = st.radio("ì´ ì´ë¯¸ì§€ê°€ ë§ˆìŒì— ë“œì‹œë‚˜ìš”?", ["ë„¤, ë‹¤ìŒ ê·¸ë¦¼ìœ¼ë¡œ ë„˜ì–´ê°€ê¸°", "ì•„ë‹ˆìš”, ìˆ˜ì •í•˜ê³  ì‹¶ì–´ìš”"], key="img_feedback")

        if feedback == "ì•„ë‹ˆìš”, ìˆ˜ì •í•˜ê³  ì‹¶ì–´ìš”":
            revise = st.text_input("ì–´ë–¤ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ê³  ì‹¶ë‚˜ìš”? (ì˜ˆ: ëª¨ì ìƒ‰ê¹”ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ, í‘œì •ì„ ë” ë°ê²Œ)", key="img_revise")
            if st.button("ìˆ˜ì •ëœ ì´ë¯¸ì§€ ìƒì„±") and revise:
                # Pika íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬, ìˆ˜ì • ì‚¬í•­ì„ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ì— ë‹¨ìˆœí•˜ê²Œ ì¶”ê°€
                # Considering Pika's characteristics, simply add revisions to the existing prompt
                revised_prompt = f"{st.session_state.current_prompt}, ë‹¨ {revise}"
                with st.spinner("ìˆ˜ì •ëœ ì´ë¯¸ì§€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    revised_image = generate_image(revised_prompt)
                    st.session_state.image_history.append((revised_prompt, revised_image))
                    st.experimental_rerun() # ìˆ˜ì •ëœ ì´ë¯¸ì§€ ë°”ë¡œ í‘œì‹œ
                                            # Display revised image immediately
        elif feedback == "ë„¤, ë‹¤ìŒ ê·¸ë¦¼ìœ¼ë¡œ ë„˜ì–´ê°€ê¸°":
            st.success("ë‹¤ìŒ ìºë¦­í„° ë˜ëŠ” ë°°ê²½ ì…ë ¥ ë‹¨ê³„ë¡œ ë„˜ì–´ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
            # ë‹¤ìŒ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ ìƒíƒœ ì´ˆê¸°í™” (ì„ íƒ ì‚¬í•­, í•„ìš”ì— ë”°ë¼ ìœ ì§€ ê°€ëŠ¥)
            # Reset state for next image generation (optional, can be maintained if needed)
            # st.session_state.image_history = []
            # st.session_state.current_prompt = ""


# 4. ì¥ë©´ë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ì ê²€
# 4. Scene-by-Scene Video Prompt Review
elif chat_option.startswith("4"):
    st.header("4. ì¥ë©´ë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ì ê²€")
    st.markdown("ğŸ¥ **ëª©í‘œ:** ê° ì¥ë©´ì„ Pika ì˜ìƒìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ GPTì™€ í•¨ê»˜ ìµœì¢… ì ê²€í•˜ê³  ê°„ê²°í•˜ê²Œ ì™„ì„±í•´ ë³´ì„¸ìš”.")
    st.markdown("**ğŸ’¡ íŒ:** PikaëŠ” ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë³´ë‹¤ **ì§§ê³  í•µì‹¬ì ì¸ ë¬¸ì¥**ì— ë” ì˜ ë°˜ì‘í•´ìš”! ë™ì‘, ê°ì •, êµ¬ë„ì— ì§‘ì¤‘í•˜ì„¸ìš”.")

    scene = st.text_area("ì˜ìƒìœ¼ë¡œ ë§Œë“¤ ì¥ë©´ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í† ë¼ê°€ ìˆ² ì†ì„ ë›°ì–´ë‹¤ë‹ˆë©° ë‹¹ê·¼ì„ ë°œê²¬í•œë‹¤.)")
    if st.button("GPTì—ê²Œ í”„ë¡¬í”„íŠ¸ ì ê²€ ìš”ì²­") and scene:
        messages = [
            {"role": "system", "content": (
                "ë„ˆëŠ” í•™ìƒì´ ì…ë ¥í•œ ì¥ë©´ ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ Pikaì—ì„œ ì‚¬ìš©í•  10ì´ˆ ë¶„ëŸ‰ ì˜ìƒ í”„ë¡¬í”„íŠ¸ê°€ ì ì ˆí•œì§€ ê²€í† í•˜ëŠ” GPTì•¼.\n"
                "Pikaì˜ í”„ë¡¬í”„íŠ¸ í•´ì„ íŠ¹ì„±ìƒ, **ì§§ê³  ê°„ê²°í•˜ë©° ëª…í™•í•œ 1~2ë¬¸ì¥**ì˜ í”„ë¡¬í”„íŠ¸ê°€ ì¤‘ìš”í•´. ê³¼ë„í•œ ë¬¸ì¥ ê¸¸ì´, ì„œìˆ ì ì¸ ë¬˜ì‚¬ëŠ” í”¼í•´ì•¼ í•´.\n"
                "ìºë¦­í„°ì™€ ë°°ê²½ ì´ë¯¸ì§€ëŠ” ë”°ë¡œ ìƒì„±í•˜ë¯€ë¡œ, ì´ ì°½ì—ì„œëŠ” ì¥ë©´ì˜ **ë™ì‘, ê°ì •, êµ¬ë„(ì „ì²´ ëª¸, ì‹œì ), ê°•ì¡°í•˜ê³  ì‹¶ì€ ìš”ì†Œ** ì¤‘ì‹¬ìœ¼ë¡œ ì§ˆë¬¸í•˜ê³  ë³´ì™„í•´.\n"
                "ì§ˆë¬¸ì€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ í•˜ê³ , ì •ë³´ê°€ ì¶©ë¶„í•˜ë©´ ê°„ê²°í•œ 1~2ë¬¸ì¥ í˜•íƒœë¡œ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ì¶œë ¥í•´ì¤˜. ì´ë•Œ, ë¶ˆí•„ìš”í•œ ì„œë‘ëŠ” ì œì™¸í•˜ê³  í”„ë¡¬í”„íŠ¸ë§Œ ëª…í™•íˆ ì œì‹œí•´ì•¼ í•´."
            )},
            {"role": "user", "content": scene}
        ]
        with st.spinner("GPTê°€ ì˜ìƒ í”„ë¡¬í”„íŠ¸ë¥¼ ì ê²€ ì¤‘ì…ë‹ˆë‹¤..."):
            st.write(ask_gpt(messages))
